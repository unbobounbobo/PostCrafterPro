"""
Batch Processing API Routes

è¤‡æ•°ã®æŠ•ç¨¿ã‚’ä¸€æ‹¬ã§ç”Ÿæˆãƒ»å‡¦ç†ã™ã‚‹ãŸã‚ã®API
"""

from flask import Blueprint, request, jsonify
from app.services.sheets_service import SheetsService
from app.services.claude_service import ClaudeService
from app.services.pinecone_service import PineconeService
import traceback
import csv
import io
from datetime import datetime

batch_api_bp = Blueprint('batch_api', __name__, url_prefix='/api/batch')


@batch_api_bp.route('/load', methods=['POST'])
def load_batch_data():
    """
    Step 1: Load batch data from Google Sheets

    Request:
        {
            "source": "sheet",  # "sheet", "csv", or "manual"
            "sheet_name": "SNSæŠ•ç¨¿_ä¸‹æ›¸ã",
            "start_row": 2,
            "end_row": 0  # 0 = all rows
        }

    Response:
        {
            "success": true,
            "posts": [
                {
                    "row": 2,
                    "date": "2025-01-15",
                    "url": "...",
                    "decided": "...",
                    "anniversary": "...",
                    "remarks": "..."
                },
                ...
            ],
            "count": 10
        }
    """
    try:
        data = request.get_json()
        source = data.get('source', 'sheet')

        print(f"\n{'ğŸ“¦'*30}")
        print(f"[BATCH] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        print(f"   ã‚½ãƒ¼ã‚¹: {source}")
        print(f"{'ğŸ“¦'*30}\n")

        if source == 'sheet':
            sheet_name = data.get('sheet_name', 'SNSæŠ•ç¨¿_ä¸‹æ›¸ã')
            start_row = data.get('start_row', 2)
            end_row = data.get('end_row', 0)

            print(f"[INFO] Google Sheetsã‹ã‚‰èª­ã¿è¾¼ã¿")
            print(f"   ã‚·ãƒ¼ãƒˆå: {sheet_name}")
            print(f"   é–‹å§‹è¡Œ: {start_row}")
            print(f"   çµ‚äº†è¡Œ: {end_row if end_row > 0 else 'å…¨ä»¶'}")

            sheets_service = SheetsService()

            # Get all data from the sheet
            all_values = sheets_service.draft_sheet.get_all_values()

            if not all_values or len(all_values) < 2:
                return jsonify({
                    'success': False,
                    'error': 'ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“'
                }), 400

            # Headers are in row 1 (index 0)
            headers = all_values[0]

            # Find column indices
            try:
                date_idx = headers.index('æŠ•ç¨¿æ—¥')
                url_idx = headers.index('URL')
                decided_idx = headers.index('æ±ºå®šäº‹é …')
                anniversary_idx = headers.index('è¨˜å¿µæ—¥') if 'è¨˜å¿µæ—¥' in headers else None
                remarks_idx = headers.index('è£œè¶³') if 'è£œè¶³' in headers else None
                final_post_idx = headers.index('æœ€çµ‚æŠ•ç¨¿') if 'æœ€çµ‚æŠ•ç¨¿' in headers else None
            except ValueError as e:
                return jsonify({
                    'success': False,
                    'error': f'å¿…é ˆã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}'
                }), 400

            # Extract data rows
            posts = []
            data_rows = all_values[start_row - 1:]  # start_row is 1-indexed

            if end_row > 0:
                data_rows = data_rows[:end_row - start_row + 1]

            for i, row in enumerate(data_rows):
                row_number = start_row + i

                # Skip if already has final post
                if final_post_idx and len(row) > final_post_idx and row[final_post_idx].strip():
                    print(f"[SKIP] è¡Œ{row_number}: ã™ã§ã«æœ€çµ‚æŠ•ç¨¿ãŒå­˜åœ¨")
                    continue

                # Skip if missing required fields
                if len(row) <= decided_idx or not row[decided_idx].strip():
                    print(f"[SKIP] è¡Œ{row_number}: æ±ºå®šäº‹é …ãŒç©º")
                    continue

                post = {
                    'row': row_number,
                    'date': row[date_idx] if len(row) > date_idx else '',
                    'url': row[url_idx] if len(row) > url_idx else '',
                    'decided': row[decided_idx] if len(row) > decided_idx else '',
                    'anniversary': row[anniversary_idx] if anniversary_idx and len(row) > anniversary_idx else '',
                    'remarks': row[remarks_idx] if remarks_idx and len(row) > remarks_idx else ''
                }

                posts.append(post)

            print(f"\nâœ… [æˆåŠŸ] {len(posts)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

            return jsonify({
                'success': True,
                'posts': posts,
                'count': len(posts)
            }), 200

        elif source == 'csv':
            # CSV parsing (to be implemented)
            return jsonify({
                'success': False,
                'error': 'CSVèª­ã¿è¾¼ã¿ã¯æœªå®Ÿè£…ã§ã™'
            }), 501

        else:
            return jsonify({
                'success': False,
                'error': f'ä¸æ˜ãªã‚½ãƒ¼ã‚¹: {source}'
            }), 400

    except Exception as e:
        print(f"\n{'âŒ'*30}")
        print(f"[ERROR] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")
        print(f"   ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"{'âŒ'*30}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@batch_api_bp.route('/process', methods=['POST'])
def process_batch():
    """
    Step 2: Process batch posts (generate posts one by one)

    Request:
        {
            "post": {
                "row": 2,
                "date": "2025-01-15",
                "url": "...",
                "decided": "...",
                "anniversary": "...",
                "remarks": "..."
            },
            "auto_save": false,
            "select_first": true  # Automatically select first post (post_a)
        }

    Response:
        {
            "success": true,
            "post_a": {...},
            "post_b": {...},
            "selected": "..."  # If select_first=true
        }
    """
    try:
        data = request.get_json()
        post_data = data.get('post')
        auto_save = data.get('auto_save', False)
        select_first = data.get('select_first', True)

        print(f"\n{'ğŸ”„'*30}")
        print(f"[BATCH] æŠ•ç¨¿ç”Ÿæˆé–‹å§‹")
        print(f"   è¡Œ: {post_data.get('row')}")
        print(f"   æŠ•ç¨¿æ—¥: {post_data.get('date')}")
        print(f"   æ±ºå®šäº‹é …: {post_data.get('decided')}")
        print(f"{'ğŸ”„'*30}\n")

        # Initialize services
        claude_service = ClaudeService()
        pinecone_service = PineconeService()
        sheets_service = SheetsService()

        # Step 1: Get context (Pinecone + Similar posts)
        print(f"[INFO] ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ä¸­...")

        # Pinecone search
        pinecone_results = []
        if post_data.get('url'):
            try:
                pinecone_results = pinecone_service.search_products(
                    query=post_data.get('decided', ''),
                    top_k=5
                )
                print(f"âœ… Pinecone: {len(pinecone_results)}ä»¶å–å¾—")
            except Exception as e:
                print(f"âš ï¸  Pineconeæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        # Similar posts search
        similar_posts = []
        try:
            similar_posts = sheets_service.get_similar_posts(
                query=post_data.get('decided', ''),
                top_k=3
            )
            print(f"âœ… é¡ä¼¼æŠ•ç¨¿: {len(similar_posts)}ä»¶å–å¾—")
        except Exception as e:
            print(f"âš ï¸  é¡ä¼¼æŠ•ç¨¿æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        # Step 2: Generate posts
        print(f"[INFO] æŠ•ç¨¿ç”Ÿæˆä¸­...")

        result = claude_service.generate_posts(
            date=post_data.get('date', ''),
            decided=post_data.get('decided', ''),
            url=post_data.get('url', ''),
            remarks=post_data.get('remarks', ''),
            anniversary=post_data.get('anniversary', ''),
            pinecone_context=pinecone_results,
            similar_posts=similar_posts
        )

        if 'error' in result:
            print(f"âŒ [ã‚¨ãƒ©ãƒ¼] æŠ•ç¨¿ç”Ÿæˆå¤±æ•—: {result['error']}")
            return jsonify({
                'success': False,
                'error': result['error']
            }), 500

        print(f"âœ… [æˆåŠŸ] æŠ•ç¨¿ç”Ÿæˆå®Œäº†")

        # Step 3: Auto-select first post if requested
        selected_post = None
        if select_first and result.get('post_a'):
            selected_post = result['post_a']['text']
            print(f"[INFO] è‡ªå‹•é¸æŠ: æ¡ˆA")

        # Step 4: Auto-save if requested
        if auto_save and selected_post:
            print(f"[INFO] è‡ªå‹•ä¿å­˜ä¸­...")

            save_data = {
                'ä½œæˆæ—¥æ™‚': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'æŠ•ç¨¿æ—¥': post_data.get('date', ''),
                'URL': post_data.get('url', ''),
                'æ±ºå®šäº‹é …': post_data.get('decided', ''),
                'è¨˜å¿µæ—¥': post_data.get('anniversary', ''),
                'è£œè¶³': post_data.get('remarks', ''),
                'æœ€çµ‚æŠ•ç¨¿': selected_post,
                'æ–‡å­—æ•°': len(selected_post),
                'æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯': 'âœ…' if len(selected_post) <= 140 else 'âŒ',
                'ãƒ©ã‚¦ãƒ³ãƒ‰æ•°': 1,
                'Pineconeçµæœæ•°': len(pinecone_results),
                'é¡ä¼¼æŠ•ç¨¿æ•°': len(similar_posts)
            }

            try:
                # Save to draft sheet (update existing row)
                sheets_service.save_draft_post(save_data, post_data.get('row'))

                # Save to published sheet (add new row)
                sheets_service.publish_post(save_data)

                print(f"âœ… [æˆåŠŸ] è‡ªå‹•ä¿å­˜å®Œäº†")
            except Exception as e:
                print(f"âš ï¸  è‡ªå‹•ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return jsonify({
            'success': True,
            'post_a': result.get('post_a'),
            'post_b': result.get('post_b'),
            'selected': selected_post,
            'pinecone_count': len(pinecone_results),
            'similar_count': len(similar_posts)
        }), 200

    except Exception as e:
        print(f"\n{'âŒ'*30}")
        print(f"[ERROR] ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼")
        print(f"   ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"{'âŒ'*30}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@batch_api_bp.route('/export', methods=['POST'])
def export_results():
    """
    Step 3: Export batch results as CSV

    Request:
        {
            "results": [
                {
                    "row": 2,
                    "date": "2025-01-15",
                    "status": "success",
                    "post": "..."
                },
                ...
            ]
        }

    Response:
        CSV file download
    """
    try:
        data = request.get_json()
        results = data.get('results', [])

        print(f"\n{'ğŸ’¾'*30}")
        print(f"[BATCH] çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹")
        print(f"   ä»¶æ•°: {len(results)}")
        print(f"{'ğŸ’¾'*30}\n")

        # Create CSV
        output = io.StringIO()
        writer = csv.writer(output)

        # Write headers
        writer.writerow(['è¡Œç•ªå·', 'æŠ•ç¨¿æ—¥', 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹', 'ç”Ÿæˆã•ã‚ŒãŸæŠ•ç¨¿', 'ã‚¨ãƒ©ãƒ¼'])

        # Write data
        for result in results:
            writer.writerow([
                result.get('row', ''),
                result.get('date', ''),
                result.get('status', ''),
                result.get('post', ''),
                result.get('error', '')
            ])

        csv_data = output.getvalue()
        output.close()

        print(f"âœ… [æˆåŠŸ] CSVç”Ÿæˆå®Œäº†")

        return jsonify({
            'success': True,
            'csv': csv_data,
            'filename': f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        }), 200

    except Exception as e:
        print(f"\n{'âŒ'*30}")
        print(f"[ERROR] ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼")
        print(f"   ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"{'âŒ'*30}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500
